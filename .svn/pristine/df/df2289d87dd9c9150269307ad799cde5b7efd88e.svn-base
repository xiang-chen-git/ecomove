{
    "data" : [
        "par(lwd = 2, col=2, bty=\"n\", mfrow=c(2,2), oma=c(0,0,5,0), xaxt=\"n\", yaxt=\"n\")",
        "> ",
        "curve(sin(x), xlim=c(0,pi), main=\"i'm upset\")",
        "> ",
        "curve(-sin(x), xlim=c(0,pi), main=\"i'm happy\")",
        "> ",
        "curve(-cos(x), xlim=c(0,pi), main=\"i'm not sure\")",
        "> ",
        "curve(cos(x), xlim=c(-10*pi,10*pi), ylim=c(-4,4), main=\"i'm a scallop\")",
        "> ",
        "title(\"Trigonometric Emoticons\", outer=TRUE, cex=3)",
        "> ",
        "?title",
        "> ",
        "title(\"Trigonometric Emoticons\", outer=TRUE, cex.main=3)",
        "> ",
        "par(lwd = 2, col=2, bty=\"n\", mfrow=c(2,2), oma=c(0,0,5,0), xaxt=\"n\", yaxt=\"n\")",
        "> ",
        "curve(sin(x), xlim=c(0,pi), main=\"i'm upset\")",
        "> ",
        "curve(-sin(x), xlim=c(0,pi), main=\"i'm happy\")",
        "> ",
        "curve(-cos(x), xlim=c(0,pi), main=\"i'm not sure\")",
        "> ",
        "curve(cos(x), xlim=c(-10*pi,10*pi), ylim=c(-4,4), main=\"i'm a scallop\")",
        "> ",
        "title(\"Trigonometric Emoticons\", outer=TRUE, cex.main=2)",
        "> ",
        "?title",
        "> ",
        "par(lwd = 2, col=2, bty=\"n\", mfrow=c(2,2), oma=c(0,0,5,0))",
        "> ",
        "curve(sin(x), xlim=c(0,pi), main=\"i'm upset\")",
        "> ",
        "curve(-sin(x), xlim=c(0,pi), main=\"i'm happy\")",
        "> ",
        "curve(-cos(x), xlim=c(0,pi), main=\"i'm not sure\")",
        "> ",
        "curve(cos(x), xlim=c(-10*pi,10*pi), ylim=c(-4,4), main=\"i'm a scallop\")",
        "> ",
        "title(\"Trigonometric Emoticons\", outer=TRUE, cex.main=2)",
        "> ",
        "?title",
        "> ",
        "par(lwd = 2, col=2, bty=\"n\", mfrow=c(2,2), oma=c(0,0,5,0))",
        "> ",
        "curve(sin(x), xlim=c(0,pi), main=\"i'm upset\")",
        "> ",
        "curve(-sin(x), xlim=c(0,pi), main=\"i'm happy\")",
        "> ",
        "curve(-cos(x), xlim=c(0,pi), main=\"i'm not sure\")",
        "> ",
        "curve(cos(x), xlim=c(-10*pi,10*pi), ylim=c(-4,4), main=\"i'm a scallop\")",
        "> ",
        "title(\"Trigonometric Emoticons\", outer=TRUE, cex.main=2)",
        "> ",
        "?title",
        "> ",
        "par(lwd = 2, col=2, bty=\"n\", mfrow=c(2,2), oma=c(0,0,5,0))",
        "> ",
        "curve(sin(x), xlim=c(0,pi), main=\"i'm upset\")",
        "> ",
        "curve(-sin(x), xlim=c(0,pi), main=\"i'm happy\")",
        "> ",
        "curve(-cos(x), xlim=c(0,pi), main=\"i'm not sure\")",
        "> ",
        "curve(cos(x), xlim=c(-10*pi,10*pi), ylim=c(-4,4), main=\"i'm a scallop\")",
        "> ",
        "title(\"Trigonometric Emoticons\", outer=TRUE, cex.main=2)",
        "> ",
        "par(lwd = 2, col=2, bty=\"n\", mfrow=c(2,2), oma=c(0,0,5,0), xaxt=\"n\", yaxt=\"n\")",
        "> ",
        "curve(sin(x), xlim=c(0,pi), main=\"i'm upset\")",
        "> ",
        "curve(-sin(x), xlim=c(0,pi), main=\"i'm happy\")",
        "> ",
        "curve(-cos(x), xlim=c(0,pi), main=\"i'm not sure\")",
        "> ",
        "curve(cos(x), xlim=c(-10*pi,10*pi), ylim=c(-4,4), main=\"i'm a scallop\")",
        "> ",
        "title(\"Trigonometric Emoticons\", outer=TRUE, cex.main=2)",
        "> ",
        "par(lwd = 2, col=2, bty=\"n\", mfrow=c(2,2), oma=c(0,0,5,0), xaxt=\"n\", yaxt=\"n\")",
        "> ",
        "curve(sin(x), xlim=c(0,pi), main=\"i'm upset\")",
        "> ",
        "curve(-sin(x), xlim=c(0,pi), main=\"i'm happy\")",
        "> ",
        "curve(-cos(x), xlim=c(0,pi), main=\"i'm not sure\")",
        "> ",
        "curve(cos(x), xlim=c(-10*pi,10*pi), ylim=c(-4,4), main=\"i'm a scallop\")",
        "> ",
        "title(\"Trigonometric Emoticons\", outer=TRUE, cex.main=2)",
        "> ",
        "curve(-cos(x/pi), main=\"i'm not sure\")",
        "> ",
        "curve(-cos(x/(2*pi)), xlim=c(0,pi), main=\"i'm not sure\")",
        "> ",
        "curve(-cos(x/(2*pi)), main=\"i'm not sure\")",
        "> ",
        "curve(-cos(x/(pi)), main=\"i'm not sure\")",
        "> ",
        "curve(-cos(x/(2*pi)), main=\"i'm not sure\")",
        "> ",
        "curve(cos(10*x), xlim=c(0,pi), main=\"i'm a scallop\")",
        "> ",
        "curve(cos(20*x), xlim=c(0,pi), main=\"i'm a scallop\")",
        "> ",
        "curve(cos(20*x), xlim=c(0,pi), ylim=c(-4,4), main=\"i'm a scallop\")",
        "> ",
        "par(lwd = 2, col=2, bty=\"n\", mfrow=c(2,2), oma=c(0,0,5,0), xaxt=\"n\", yaxt=\"n\")",
        "> ",
        "curve(sin(x), xlim=c(0,pi), main=\"i'm upset\")",
        "> ",
        "curve(-sin(x), xlim=c(0,pi), main=\"i'm happy\")",
        "> ",
        "curve(-cos(x), xlim=c(0,pi), main=\"i'm not sure\")",
        "> ",
        "curve(cos(20*x), xlim=c(0,pi), ylim=c(-4,4), main=\"i'm a scallop\")",
        "> ",
        "title(\"Trigonometric Emoticons\", outer=TRUE, cex.main=2)",
        "> ",
        "v0.null <- diff(Z[1:2])/diff(T[1:2])",
        "> ",
        "v0x.null <- Re(v0.null)",
        "> ",
        "v0y.null <- Im(v0.null)",
        "> ",
        "tau.null <- mean(diff(T))",
        "> ",
        "nu.null <- mean(Mod(diff(Z))/diff(T))",
        "> ",
        "Z.fit <- optim(c(nu.null,tau.null,v0x.null,v0y.null), Z.like2D, Z=Re(Z[-1]), T=T[-1], hessian=TRUE,",
        "+ ",
        "                   method = \"L-BFGS-B\", lower = c(0,0,-Inf), upper = c(Inf,Inf,Inf),",
        "+ ",
        "                   control = list(fnscale = -1))",
        "> ",
        "Z.fit",
        "$par\n[1]  8.945656  2.035825 20.376597 20.376698\n\n$value\n[1] -127.3418\n\n$counts\nfunction gradient \n      31       31 \n\n$convergence\n[1] 0\n\n$message\n[1] \"CONVERGENCE: REL_REDUCTION_OF_F <= FACTR*EPSMCH\"\n\n$hessian\n              [,1]        [,2]          [,3]          [,4]\n[1,] -2.478874e+00  3.22548355  2.603429e-05  5.933209e-05\n[2,]  3.225484e+00 -8.46284164 -9.519256e-02 -9.574475e-02\n[3,]  2.603429e-05 -0.09519256 -7.121040e-02  1.776357e-09\n[4,]  5.933209e-05 -0.09574475  1.776357e-09 -7.125209e-02\n\n",
        "> ",
        "v0.null",
        "[1] 16.38893+0.76876i\n",
        "> ",
        "v0x.null <- Re(v0.null)",
        "> ",
        "v0y.null <- Im(v0.null)",
        "> ",
        "Z.like2D <-",
        "+ ",
        "  function(p, Z, T, v0=NULL)",
        "+ ",
        "  {",
        "+ ",
        "    nu <- p[1] ",
        "+ ",
        "    tau <- p[2] ",
        "+ ",
        "    if(is.null(v0)) { v0x <- p[3]; v0y <- p[4] }",
        "+ ",
        "    ",
        "+ ",
        "    Sigma.zz <- getSigma.ZZ(T, nu, tau)",
        "+ ",
        "    mux <- v0x * tau*(1-exp(-T/tau))",
        "+ ",
        "    muy <- v0y * tau*(1-exp(-T/tau))",
        "+ ",
        "    ",
        "+ ",
        "    dmvnorm2(Z,mux,Sigma.zz, log=TRUE) + dmvnorm2(Z,muy,Sigma.zz, log=TRUE)",
        "+ ",
        "  }",
        "> ",
        "Z.like2D",
        "function(p, Z, T, v0=NULL)\n  {\n    nu <- p[1] \n    tau <- p[2] \n    if(is.null(v0)) { v0x <- p[3]; v0y <- p[4] }\n    \n    Sigma.zz <- getSigma.ZZ(T, nu, tau)\n    mux <- v0x * tau*(1-exp(-T/tau))\n    muy <- v0y * tau*(1-exp(-T/tau))\n    \n    dmvnorm2(Z,mux,Sigma.zz, log=TRUE) + dmvnorm2(Z,muy,Sigma.zz, log=TRUE)\n  }\n",
        "> ",
        "Z.fit <- optim(c(nu.null,tau.null,v0x.null,v0y.null), Z.like2D, Z=Z[-1], T=T[-1], hessian=TRUE,",
        "+ ",
        "                   method = \"L-BFGS-B\", lower = c(0,0,-Inf), upper = c(Inf,Inf,Inf),",
        "+ ",
        "                   control = list(fnscale = -1))",
        "There were 50 or more warnings (use warnings() to see the first 50)\n",
        "> ",
        "Z.fit",
        "$par\n[1]  8.945656  2.035825 20.376597 20.376698\n\n$value\n[1] -127.3418\n\n$counts\nfunction gradient \n      31       31 \n\n$convergence\n[1] 0\n\n$message\n[1] \"CONVERGENCE: REL_REDUCTION_OF_F <= FACTR*EPSMCH\"\n\n$hessian\n              [,1]        [,2]          [,3]          [,4]\n[1,] -2.478874e+00  3.22548355  2.603429e-05  5.933209e-05\n[2,]  3.225484e+00 -8.46284164 -9.519256e-02 -9.574475e-02\n[3,]  2.603429e-05 -0.09519256 -7.121040e-02  1.776357e-09\n[4,]  5.933209e-05 -0.09574475  1.776357e-09 -7.125209e-02\n\n",
        "> ",
        "Z.like2D",
        "function(p, Z, T, v0=NULL)\n  {\n    nu <- p[1] \n    tau <- p[2] \n    if(is.null(v0)) { v0x <- p[3]; v0y <- p[4] }\n    \n    Sigma.zz <- getSigma.ZZ(T, nu, tau)\n    mux <- v0x * tau*(1-exp(-T/tau))\n    muy <- v0y * tau*(1-exp(-T/tau))\n    \n    dmvnorm2(Z,mux,Sigma.zz, log=TRUE) + dmvnorm2(Z,muy,Sigma.zz, log=TRUE)\n  }\n",
        "> ",
        "Z.fit <- optim(c(nu.null,tau.null,v0x.null,v0y.null), Z.like2D, Z=Z[-1], T=T[-1], v0=TRUE, hessian=TRUE,",
        "+ ",
        "                   method = \"L-BFGS-B\", lower = c(0,0,-Inf, -Inf), upper = c(Inf,Inf,Inf, Inf),",
        "+ ",
        "                   control = list(fnscale = -1))",
        "There were 50 or more warnings (use warnings() to see the first 50)\n",
        "> ",
        "Z.fit",
        "$par\n[1]  9.6092663  1.6605183 16.3889332  0.7687616\n\n$value\n[1] -140.0186\n\n$counts\nfunction gradient \n      12       12 \n\n$convergence\n[1] 0\n\n$message\n[1] \"CONVERGENCE: REL_REDUCTION_OF_F <= FACTR*EPSMCH\"\n\n$hessian\n          [,1]       [,2] [,3] [,4]\n[1,] -2.191975   3.151285    0    0\n[2,]  3.151285 -11.122794    0    0\n[3,]  0.000000   0.000000    0    0\n[4,]  0.000000   0.000000    0    0\n\n",
        "> ",
        "nu",
        "[1] 10\n",
        "> ",
        "tay",
        "Error: object 'tay' not found\n",
        "> ",
        "ta",
        "Error: object 'ta' not found\n",
        "> ",
        "tau",
        "[1] 3\n",
        "> ",
        "warnings90",
        "Error: object 'warnings90' not found\n",
        "> ",
        "warnings()",
        "Warning messages:\n1: In Mahalanobis(t(x), t(mean), sigma) : imaginary parts discarded in coercion\n2: In Mahalanobis(t(x), t(mean), sigma) : imaginary parts discarded in coercion\n3: In Mahalanobis(t(x), t(mean), sigma) : imaginary parts discarded in coercion\n4: In Mahalanobis(t(x), t(mean), sigma) : imaginary parts discarded in coercion\n5: In Mahalanobis(t(x), t(mean), sigma) : imaginary parts discarded in coercion\n6: In Mahalanobis(t(x), t(mean), sigma) : imaginary parts discarded in coercion\n7: In Mahalanobis(t(x), t(mean), sigma) : imaginary parts discarded in coercion",
        "\n8: In Mahalanobis(t(x), t(mean), sigma) : imaginary parts discarded in coercion\n9: In Mahalanobis(t(x), t(mean), sigma) : imaginary parts discarded in coercion\n10: In Mahalanobis(t(x), t(mean), sigma) : imaginary parts discarded in coercion\n11: In Mahalanobis(t(x), t(mean), sigma) : imaginary parts discarded in coercion\n12: In Mahalanobis(t(x), t(mean), sigma) : imaginary parts discarded in coercion\n13: In Mahalanobis(t(x), t(mean), sigma) : imaginary parts discarded in coercion\n14: In Mahalanobis(t(x), t(mean), sigma) : imaginary parts discarded in coercion",
        "\n15: In Mahalanobis(t(x), t(mean), sigma) : imaginary parts discarded in coercion\n16: In Mahalanobis(t(x), t(mean), sigma) : imaginary parts discarded in coercion\n17: In Mahalanobis(t(x), t(mean), sigma) : imaginary parts discarded in coercion\n18: In Mahalanobis(t(x), t(mean), sigma) : imaginary parts discarded in coercion\n19: In Mahalanobis(t(x), t(mean), sigma) : imaginary parts discarded in coercion\n20: In Mahalanobis(t(x), t(mean), sigma) : imaginary parts discarded in coercion\n21: In Mahalanobis(t(x), t(mean), sigma) : imaginary parts discarded in coercion",
        "\n22: In Mahalanobis(t(x), t(mean), sigma) : imaginary parts discarded in coercion\n23: In Mahalanobis(t(x), t(mean), sigma) : imaginary parts discarded in coercion\n24: In Mahalanobis(t(x), t(mean), sigma) : imaginary parts discarded in coercion\n25: In Mahalanobis(t(x), t(mean), sigma) : imaginary parts discarded in coercion\n26: In Mahalanobis(t(x), t(mean), sigma) : imaginary parts discarded in coercion\n27: In Mahalanobis(t(x), t(mean), sigma) : imaginary parts discarded in coercion\n28: In Mahalanobis(t(x), t(mean), sigma) : imaginary parts discarded in coercion",
        "\n29: In Mahalanobis(t(x), t(mean), sigma) : imaginary parts discarded in coercion\n30: In Mahalanobis(t(x), t(mean), sigma) : imaginary parts discarded in coercion\n31: In Mahalanobis(t(x), t(mean), sigma) : imaginary parts discarded in coercion\n32: In Mahalanobis(t(x), t(mean), sigma) : imaginary parts discarded in coercion\n33: In Mahalanobis(t(x), t(mean), sigma) : imaginary parts discarded in coercion\n34: In Mahalanobis(t(x), t(mean), sigma) : imaginary parts discarded in coercion\n35: In Mahalanobis(t(x), t(mean), sigma) : imaginary parts discarded in coercion",
        "\n36: In Mahalanobis(t(x), t(mean), sigma) : imaginary parts discarded in coercion\n37: In Mahalanobis(t(x), t(mean), sigma) : imaginary parts discarded in coercion\n38: In Mahalanobis(t(x), t(mean), sigma) : imaginary parts discarded in coercion\n39: In Mahalanobis(t(x), t(mean), sigma) : imaginary parts discarded in coercion\n40: In Mahalanobis(t(x), t(mean), sigma) : imaginary parts discarded in coercion\n41: In Mahalanobis(t(x), t(mean), sigma) : imaginary parts discarded in coercion\n42: In Mahalanobis(t(x), t(mean), sigma) : imaginary parts discarded in coercion",
        "\n43: In Mahalanobis(t(x), t(mean), sigma) : imaginary parts discarded in coercion\n44: In Mahalanobis(t(x), t(mean), sigma) : imaginary parts discarded in coercion\n45: In Mahalanobis(t(x), t(mean), sigma) : imaginary parts discarded in coercion\n46: In Mahalanobis(t(x), t(mean), sigma) : imaginary parts discarded in coercion\n47: In Mahalanobis(t(x), t(mean), sigma) : imaginary parts discarded in coercion\n48: In Mahalanobis(t(x), t(mean), sigma) : imaginary parts discarded in coercion\n49: In Mahalanobis(t(x), t(mean), sigma) : imaginary parts discarded in coercion",
        "\n50: In Mahalanobis(t(x), t(mean), sigma) : imaginary parts discarded in coercion\n",
        "> ",
        "Z.like2D <-",
        "+ ",
        "  function(p, Z, T, v0=NULL)",
        "+ ",
        "  {",
        "+ ",
        "    nu <- p[1] ",
        "+ ",
        "    tau <- p[2] ",
        "+ ",
        "    if(is.null(v0)) { v0x <- p[3]; v0y <- p[4] }",
        "+ ",
        "    ",
        "+ ",
        "    Sigma.zz <- getSigma.ZZ(T, nu, tau)",
        "+ ",
        "    mux <- v0x * tau*(1-exp(-T/tau))",
        "+ ",
        "    muy <- v0y * tau*(1-exp(-T/tau))",
        "+ ",
        "    ",
        "+ ",
        "    dmvnorm2(Re(Z),mux,Sigma.zz, log=TRUE) + dmvnorm2(Im(Z),muy,Sigma.zz, log=TRUE)",
        "+ ",
        "  }",
        "> ",
        "Z.fit <- optim(c(nu.null,tau.null,v0x.null,v0y.null), Z.like2D, Z=Z[-1], T=T[-1], v0=TRUE, hessian=TRUE,",
        "+ ",
        "                   method = \"L-BFGS-B\", lower = c(0,0,-Inf, -Inf), upper = c(Inf,Inf,Inf, Inf),",
        "+ ",
        "                   control = list(fnscale = -1))",
        "> ",
        "Z.fit",
        "$par\n[1]  8.7089912  2.1428305 16.3889332  0.7687616\n\n$value\n[1] -123.1548\n\n$counts\nfunction gradient \n      49       49 \n\n$convergence\n[1] 52\n\n$message\n[1] \"ERROR: ABNORMAL_TERMINATION_IN_LNSRCH\"\n\n$hessian\n          [,1]      [,2] [,3] [,4]\n[1,] -2.639795  3.153331    0    0\n[2,]  3.153331 -7.396752    0    0\n[3,]  0.000000  0.000000    0    0\n[4,]  0.000000  0.000000    0    0\n\n",
        "> ",
        "Z.fit <- optim(c(nu.null,tau.null), Z.like2D, Z=Z[-1], T=T[-1], hessian=TRUE,",
        "+ ",
        "                     method = \"L-BFGS-B\", lower = c(0,0), upper = c(Inf,Inf),",
        "+ ",
        "                     control = list(fnscale = -1))",
        "Error in optim(c(nu.null, tau.null), Z.like2D, Z = Z[-1], T = T[-1], hessian = TRUE,  : \n  L-BFGS-B needs finite values of 'fn'\n",
        "> ",
        "Z.like2D",
        "function(p, Z, T, v0=NULL)\n  {\n    nu <- p[1] \n    tau <- p[2] \n    if(is.null(v0)) { v0x <- p[3]; v0y <- p[4] }\n    \n    Sigma.zz <- getSigma.ZZ(T, nu, tau)\n    mux <- v0x * tau*(1-exp(-T/tau))\n    muy <- v0y * tau*(1-exp(-T/tau))\n    \n    dmvnorm2(Re(Z),mux,Sigma.zz, log=TRUE) + dmvnorm2(Im(Z),muy,Sigma.zz, log=TRUE)\n  }\n",
        "> ",
        "Z.like2D <-",
        "+ ",
        "  function(p, Z, T, v0=NULL)",
        "+ ",
        "  {",
        "+ ",
        "    nu <- p[1] ",
        "+ ",
        "    tau <- p[2] ",
        "+ ",
        "    if(is.null(v0)) { v0x <- p[3]; v0y <- p[4] } else{v0x <- Re(v0); v0y <- Im(v0)}",
        "+ ",
        "    ",
        "+ ",
        "    Sigma.zz <- getSigma.ZZ(T, nu, tau)",
        "+ ",
        "    mux <- v0x * tau*(1-exp(-T/tau))",
        "+ ",
        "    muy <- v0y * tau*(1-exp(-T/tau))",
        "+ ",
        "    ",
        "+ ",
        "    dmvnorm2(Re(Z),mux,Sigma.zz, log=TRUE) + dmvnorm2(Im(Z),muy,Sigma.zz, log=TRUE)",
        "+ ",
        "  }",
        "> ",
        "Z.fit <- optim(c(nu.null,tau.null), Z.like2D, Z=Z[-1], T=T[-1], v0 = v0.null, hessian=TRUE,",
        "+ ",
        "                     method = \"L-BFGS-B\", lower = c(0,0), upper = c(Inf,Inf),",
        "+ ",
        "                     control = list(fnscale = -1))",
        "> ",
        "Z.fit",
        "$par\n[1] 8.831413 2.194516\n\n$value\n[1] -123.8355\n\n$counts\nfunction gradient \n      10       10 \n\n$convergence\n[1] 0\n\n$message\n[1] \"CONVERGENCE: REL_REDUCTION_OF_F <= FACTR*EPSMCH\"\n\n$hessian\n          [,1]      [,2]\n[1,] -2.565331  3.146735\n[2,]  3.146735 -7.030446\n\n",
        "> ",
        "Z.fit$par",
        "[1] 8.831413 2.194516\n",
        "> ",
        "nu",
        "[1] 10\n",
        "> ",
        "nu.se <- sqrt(-1/Z.fit$hessian[1,1])",
        "> ",
        "nu.se",
        "[1] 0.6243503\n",
        "> ",
        "qnorm(0.975)",
        "[1] 1.959964\n",
        "> ",
        "tau.hat <- Z.fit$par[2]",
        "> ",
        "tau.se <- sqrt(-1/Z.fit$hessian[2,2])",
        "> ",
        "tau.CI <- nu.hat + c(-1,1)*qnorm(0.975)*nu.se",
        "Error: object 'nu.hat' not found\n",
        "> ",
        "tau.hat <- Z.fit$par[2]",
        "> ",
        "tau.se <- sqrt(-1/Z.fit$hessian[2,2])",
        "> ",
        "tau.CI <- tau.hat + c(-1,1)*qnorm(0.975)*nu.se",
        "> ",
        "results <- data.frame(t(results), rbind(tau.CI, nu.CI))",
        "Error in t(results) : \n  error in evaluating the argument 'x' in selecting a method for function 't': Error: object 'results' not found\n",
        "> ",
        "results <- data.frame(c(nu.hat, tau.hat)), rbind(tau.CI, nu.CI))",
        "Error: unexpected ',' in \"results <- data.frame(c(nu.hat, tau.hat)),\"\n",
        "> ",
        "results <- data.frame(c(nu.hat, tau.hat), rbind(tau.CI, nu.CI))",
        "Error in data.frame(c(nu.hat, tau.hat), rbind(tau.CI, nu.CI)) : \n  object 'nu.hat' not found\n",
        "> ",
        "nu.hat <- Z.fit$par[1]",
        "> ",
        "nu.se <- sqrt(-1/Z.fit$hessian[1,1])",
        "> ",
        "nu.CI <- nu.hat + c(-1,1)*qnorm(0.975)*nu.se",
        "> ",
        "# tau estimates",
        "> ",
        "tau.hat <- Z.fit$par[2]",
        "> ",
        "tau.se <- sqrt(-1/Z.fit$hessian[2,2])",
        "> ",
        "tau.CI <- tau.hat + c(-1,1)*qnorm(0.975)*nu.se",
        "> ",
        "# compile results",
        "> ",
        "results <- data.frame(c(nu.hat, tau.hat), rbind(tau.CI, nu.CI))",
        "> ",
        "results",
        "       c.nu.hat..tau.hat.        X1       X2\ntau.CI           8.831413 0.9708115  3.41822\nnu.CI            2.194516 7.6077094 10.05512\n",
        "> ",
        "nu.se <- sqrt(-1/Z.fit$hessian[1,1])",
        "> ",
        "nu.se",
        "[1] 0.6243503\n",
        "> ",
        "# nu estimates",
        "> ",
        "    nu.hat <- Z.fit$par[1]",
        "> ",
        "    nu.se <- sqrt(-1/Z.fit$hessian[1,1])",
        "> ",
        "    nu.CI <- nu.hat + c(-1,1)*qnorm(0.975)*nu.se",
        "> ",
        "    ",
        "> ",
        "    # tau estimates",
        "> ",
        "    tau.hat <- Z.fit$par[2]",
        "> ",
        "    tau.se <- sqrt(-1/Z.fit$hessian[2,2])",
        "> ",
        "    tau.CI <- tau.hat + c(-1,1)*qnorm(0.975)*tau.se",
        "> ",
        "    ",
        "> ",
        "    # compile results",
        "> ",
        "    results <- data.frame(c(nu.hat, tau.hat), rbind(tau.CI, nu.CI))",
        "> ",
        "    row.names(results) <- c(\"tau\", \"nu\")",
        "> ",
        "    names(results) <- c(\"Estimate\", \"C.I.low\", \"C.I.high\")",
        "> ",
        "results",
        "    Estimate  C.I.low  C.I.high\ntau 8.831413 1.455325  2.933707\nnu  2.194516 7.607709 10.055118\n",
        "> ",
        "v0y.hat <- Z.fit$par[4]",
        "> ",
        "v0y.se <- sqrt(-1/Z.fit$hessian[4,4])",
        "Error in Z.fit$hessian[4, 4] : subscript out of bounds\n",
        "> ",
        "v0y.CI <- v0y.hat + c(-1,1)*qnorm(0.975)*v0y.se",
        "Error: object 'v0y.se' not found\n",
        "> ",
        "Z.fit$par",
        "[1] 8.831413 2.194516\n",
        "> ",
        "Z.fit$par[4]",
        "[1] NA\n",
        "> ",
        "Z.fit <- optim(c(nu.null,tau.null,v0x.null,v0y.null), Z.like2D, Z=Z[-1], T=T[-1], v0=TRUE, hessian=TRUE,",
        "+ ",
        "                   method = \"L-BFGS-B\", lower = c(0,0,-Inf, -Inf), upper = c(Inf,Inf,Inf, Inf),",
        "+ ",
        "                   control = list(fnscale = -1))",
        "> ",
        "Z.fit",
        "$par\n[1]  9.3094122  1.7310504 16.3889332  0.7687616\n\n$value\n[1] -135.7854\n\n$counts\nfunction gradient \n      12       12 \n\n$convergence\n[1] 0\n\n$message\n[1] \"CONVERGENCE: REL_REDUCTION_OF_F <= FACTR*EPSMCH\"\n\n$hessian\n          [,1]      [,2] [,3] [,4]\n[1,] -2.316467  3.207497    0    0\n[2,]  3.207497 -9.972177    0    0\n[3,]  0.000000  0.000000    0    0\n[4,]  0.000000  0.000000    0    0\n\n",
        "> ",
        "v0x.hat <- Z.fit$par[3]",
        "> ",
        "v0x.se <- sqrt(-1/Z.fit$hessian[3,3])",
        "> ",
        "v0x.se",
        "[1] Inf\n",
        "> ",
        "v0x.CI <- v0x.hat + c(-1,1)*qnorm(0.975)*v0x.se",
        "> ",
        "v0y.hat <- Z.fit$par[4]",
        "> ",
        "v0y.se <- sqrt(-1/Z.fit$hessian[4,4])",
        "> ",
        "v0y.CI <- v0y.hat + c(-1,1)*qnorm(0.975)*v0y.se",
        "> ",
        "results <- rbind(results,",
        "+ ",
        "v0x = c(v0x.hat, v0x.CI),",
        "+ ",
        "v0y = c(v0y.hat, v0y.CI))",
        "> ",
        "results",
        "      Estimate  C.I.low  C.I.high\ntau  8.8314135 1.455325  2.933707\nnu   2.1945156 7.607709 10.055118\nv0x 16.3889332     -Inf       Inf\nv0y  0.7687616     -Inf       Inf\n",
        "> ",
        "t(results[,1])",
        "         [,1]     [,2]     [,3]      [,4]\n[1,] 8.831413 2.194516 16.38893 0.7687616\n",
        "> ",
        "results <- data.frame(t(results[,1]))",
        "> ",
        "results",
        "        X1       X2       X3        X4\n1 8.831413 2.194516 16.38893 0.7687616\n",
        "> ",
        "data.frame(t(results[,1]))",
        "  t.results...1..\n1        8.831413\n",
        "> ",
        "nu.hat <- Z.fit$par[1]",
        "> ",
        "    nu.se <- sqrt(-1/Z.fit$hessian[1,1])",
        "> ",
        "    nu.CI <- nu.hat + c(-1,1)*qnorm(0.975)*nu.se",
        "> ",
        "    ",
        "> ",
        "    # tau estimates",
        "> ",
        "    tau.hat <- Z.fit$par[2]",
        "> ",
        "    tau.se <- sqrt(-1/Z.fit$hessian[2,2])",
        "> ",
        "    tau.CI <- tau.hat + c(-1,1)*qnorm(0.975)*tau.se",
        "> ",
        "    ",
        "> ",
        "    # compile results",
        "> ",
        "    results <- data.frame(c(nu.hat, tau.hat), rbind(tau.CI, nu.CI))",
        "> ",
        "    row.names(results) <- c(\"tau\", \"nu\")",
        "> ",
        "    names(results) <- c(\"Estimate\", \"C.I.low\", \"C.I.high\")",
        "> ",
        "data.frame(t(results[,1]))",
        "        X1      X2\n1 9.309412 1.73105\n",
        "> ",
        "# v0x estimates",
        "> ",
        "      v0x.hat <- Z.fit$par[3]",
        "> ",
        "      v0x.se <- sqrt(-1/Z.fit$hessian[3,3])",
        "> ",
        "      v0x.CI <- v0x.hat + c(-1,1)*qnorm(0.975)*v0x.se",
        "> ",
        "      ",
        "> ",
        "      # v0y estimates",
        "> ",
        "      v0y.hat <- Z.fit$par[4]",
        "> ",
        "      v0y.se <- sqrt(-1/Z.fit$hessian[4,4])",
        "> ",
        "      v0y.CI <- v0y.hat + c(-1,1)*qnorm(0.975)*v0y.se",
        "> ",
        "      ",
        "> ",
        "      results <- rbind(results, ",
        "+ ",
        "                       v0x = c(v0x.hat, v0x.CI),",
        "+ ",
        "                       v0y = c(v0y.hat, v0y.CI))",
        "> ",
        "results <- data.frame(t(results[,1]))",
        "> ",
        "results",
        "        X1      X2       X3        X4\n1 9.309412 1.73105 16.38893 0.7687616\n",
        "> ",
        "# v0x estimates",
        "> ",
        "      v0x.hat <- Z.fit$par[3]",
        "> ",
        "      v0x.se <- sqrt(-1/Z.fit$hessian[3,3])",
        "> ",
        "      v0x.CI <- v0x.hat + c(-1,1)*qnorm(0.975)*v0x.se",
        "> ",
        "      ",
        "> ",
        "      # v0y estimates",
        "> ",
        "      v0y.hat <- Z.fit$par[4]",
        "> ",
        "      v0y.se <- sqrt(-1/Z.fit$hessian[4,4])",
        "> ",
        "      v0y.CI <- v0y.hat + c(-1,1)*qnorm(0.975)*v0y.se",
        "> ",
        "      ",
        "> ",
        "      results <- rbind(results, ",
        "+ ",
        "                       v0x = c(v0x.hat, v0x.CI),",
        "+ ",
        "                       v0y = c(v0y.hat, v0y.CI))",
        "> ",
        "results",
        "            X1      X2       X3         X4\n1    9.3094122 1.73105 16.38893  0.7687616\nv0x 16.3889332    -Inf      Inf 16.3889332\nv0y  0.7687616    -Inf      Inf  0.7687616\n",
        "> ",
        "# nu estimates",
        "> ",
        "    nu.hat <- Z.fit$par[1]",
        "> ",
        "    nu.se <- sqrt(-1/Z.fit$hessian[1,1])",
        "> ",
        "    nu.CI <- nu.hat + c(-1,1)*qnorm(0.975)*nu.se",
        "> ",
        "    ",
        "> ",
        "    # tau estimates",
        "> ",
        "    tau.hat <- Z.fit$par[2]",
        "> ",
        "    tau.se <- sqrt(-1/Z.fit$hessian[2,2])",
        "> ",
        "    tau.CI <- tau.hat + c(-1,1)*qnorm(0.975)*tau.se",
        "> ",
        "    ",
        "> ",
        "    # compile results",
        "> ",
        "    results <- data.frame(c(nu.hat, tau.hat), rbind(tau.CI, nu.CI))",
        "> ",
        "    row.names(results) <- c(\"tau\", \"nu\")",
        "> ",
        "    names(results) <- c(\"Estimate\", \"C.I.low\", \"C.I.high\")",
        "> ",
        "# v0x estimates",
        "> ",
        "      v0x.hat <- Z.fit$par[3]",
        "> ",
        "      v0x.se <- sqrt(-1/Z.fit$hessian[3,3])",
        "> ",
        "      v0x.CI <- v0x.hat + c(-1,1)*qnorm(0.975)*v0x.se",
        "> ",
        "      ",
        "> ",
        "      # v0y estimates",
        "> ",
        "      v0y.hat <- Z.fit$par[4]",
        "> ",
        "      v0y.se <- sqrt(-1/Z.fit$hessian[4,4])",
        "> ",
        "      v0y.CI <- v0y.hat + c(-1,1)*qnorm(0.975)*v0y.se",
        "> ",
        "      ",
        "> ",
        "      results <- rbind(results, ",
        "+ ",
        "                       v0x = c(v0x.hat, v0x.CI),",
        "+ ",
        "                       v0y = c(v0y.hat, v0y.CI))",
        "> ",
        "results",
        "      Estimate  C.I.low  C.I.high\ntau  9.3094122 1.110391  2.351709\nnu   1.7310504 8.021652 10.597173\nv0x 16.3889332     -Inf       Inf\nv0y  0.7687616     -Inf       Inf\n",
        "> ",
        "data.frame(t(results[,1]), names=row.names(results))",
        "        X1      X2       X3        X4 names\n1 9.309412 1.73105 16.38893 0.7687616   tau\n2 9.309412 1.73105 16.38893 0.7687616    nu\n3 9.309412 1.73105 16.38893 0.7687616   v0x\n4 9.309412 1.73105 16.38893 0.7687616   v0y\n",
        "> ",
        "?data.frame",
        "> ",
        "results",
        "      Estimate  C.I.low  C.I.high\ntau  9.3094122 1.110391  2.351709\nnu   1.7310504 8.021652 10.597173\nv0x 16.3889332     -Inf       Inf\nv0y  0.7687616     -Inf       Inf\n",
        "> ",
        "t(results)",
        "              tau        nu      v0x       v0y\nEstimate 9.309412  1.731050 16.38893 0.7687616\nC.I.low  1.110391  8.021652     -Inf      -Inf\nC.I.high 2.351709 10.597173      Inf       Inf\n",
        "> ",
        "t(results)[1,]",
        "       tau         nu        v0x        v0y \n 9.3094122  1.7310504 16.3889332  0.7687616 \n\nRestarting R session...\n\n",
        "> ",
        "library(cvm)",
        "\nAttaching package: ‘cvm’\n\n",
        "The following object is masked _by_ ‘.GlobalEnv’:\n\n    Z.like2D\n\n",
        "\nRestarting R session...\n\n",
        "> ",
        "rm(Z.like2D)",
        "> ",
        "library(cvm)",
        "\nRestarting R session...\n\n",
        "> ",
        "library(cvm)",
        "> ",
        "data(CVMstan2D)",
        "> ",
        "iter=1000",
        "> ",
        "chains=4",
        "> ",
        "CVM.model",
        "Error: object 'CVM.model' not found\n",
        "> ",
        "data(CVMstan2D)",
        "> ",
        "data(CVMstan2D)",
        "> ",
        "CVM.fit <- sampling(CVMstan2D, data = CVM.data, iter = iter, pars=c(\"tau\", \"nu\"), chains = chains)",
        "Error in .local(object, ...) : object 'CVM.data' not found\n",
        "> ",
        "CVM.data <- list(N = length(Z)-1, XY = c(Re(Z)[-1], Im(Z)[-1]), T=T[-1], v0x=Re(v0), v0y = Im(v0))",
        "> ",
        "CVM.fit <- sampling(CVMstan2D, data = CVM.data, iter = iter, pars=c(\"tau\", \"nu\"), chains = chains)",
        "SAMPLING FOR MODEL 'cvmlikelihood2D' NOW (CHAIN 1).\n\rIteration:   1 / 1000 [  0%]  (Adapting)\nInformational Message: The parameter state is about to be Metropolis rejected due to the following underlying, non-fatal (really) issue (and please ignore that what comes next might say 'error'): Error in function stan::prob::multi_normal_log(d): Covariance matrix is not positive definite. Covariance matrix(0,0) is -1.#IND.\nIf the problem persists across multiple draws, you might have a problem with an initial state or a gradient somewhere.",
        "\n If the problem does not persist, the resulting samples will still be drawn from the posterior.\n\nInformational Message: The parameter state is about to be Metropolis rejected due to the following underlying, non-fatal (really) issue (and please ignore that what comes next might say 'error'): Error in function stan::prob::multi_normal_log(d): Covariance matrix is not positive definite. Covariance matrix(0,0) is -1.#IND.\nIf the problem persists across multiple draws, you might have a problem with an initial state or a gradient somewhere.",
        "\n If the problem does not persist, the resulting samples will still be drawn from the posterior.\n\nInformational Message: The parameter state is about to be Metropolis rejected due to the following underlying, non-fatal (really) issue (and please ignore that what comes next might say 'error'): Error in function stan::prob::multi_normal_log(d): Covariance matrix is not positive definite. Covariance matrix(0,0) is -1.#IND.\nIf the problem persists across multiple draws, you might have a problem with an initial state or a gradient somewhere.",
        "\n If the problem does not persist, the resulting samples will still be drawn from the posterior.\n\nInformational Message: The parameter state is about to be Metropolis rejected due to the following underlying, non-fatal (really) issue (and please ignore that what comes next might say 'error'): Error in function stan::prob::multi_normal_log(d): Covariance matrix is not positive definite. Covariance matrix(0,0) is -1.7952448684541538e+057.\nIf the problem persists across multiple draws, you might have a problem with an initial state or a gradient somewhere.",
        "\n If the problem does not persist, the resulting samples will still be drawn from the posterior.\n\nInformational Message: The parameter state is about to be Metropolis rejected due to the following underlying, non-fatal (really) issue (and please ignore that what comes next might say 'error'): Error in function stan::prob::multi_normal_log(d): Covariance matrix is not positive definite. Covariance matrix(0,0) is -3584347588056288.5.\nIf the problem persists across multiple draws, you might have a problem with an initial state or a gradient somewhere.",
        "\n If the problem does not persist, the resulting samples will still be drawn from the posterior.\n\rIteration: 100 / 1000 [ 10%]  (Adapting)\rIteration: 200 / 1000 [ 20%]  (Adapting)\rIteration: 300 / 1000 [ 30%]  (Adapting)\rIteration: 400 / 1000 [ 40%]  (Adapting)\rIteration: 500 / 1000 [ 50%]  (Adapting)\rIteration: 600 / 1000 [ 60%]  (Sampling)\rIteration: 700 / 1000 [ 70%]  (Sampling)\rIteration: 800 / 1000 [ 80%]  (Sampling)\rIteration: 900 / 1000 [ 90%]  (Sampling)\rIteration: 1000 / 1000 [100%]  (Sampling)\n\nSAMPLING",
        " FOR MODEL 'cvmlikelihood2D' NOW (CHAIN 2).\n\rIteration:   1 / 1000 [  0%]  (Adapting)\nInformational Message: The parameter state is about to be Metropolis rejected due to the following underlying, non-fatal (really) issue (and please ignore that what comes next might say 'error'): Error in function stan::prob::multi_normal_log(d): Covariance matrix is not positive definite. Covariance matrix(0,0) is -1.#IND.\nIf the problem persists across multiple draws, you might have a problem with an initial state or a gradient somewhere.",
        "\n If the problem does not persist, the resulting samples will still be drawn from the posterior.\n\nInformational Message: The parameter state is about to be Metropolis rejected due to the following underlying, non-fatal (really) issue (and please ignore that what comes next might say 'error'): Error in function stan::prob::multi_normal_log(d): Covariance matrix is not positive definite. Covariance matrix(0,0) is -1.#IND.\nIf the problem persists across multiple draws, you might have a problem with an initial state or a gradient somewhere.",
        "\n If the problem does not persist, the resulting samples will still be drawn from the posterior.\n\nInformational Message: The parameter state is about to be Metropolis rejected due to the following underlying, non-fatal (really) issue (and please ignore that what comes next might say 'error'): Error in function stan::prob::multi_normal_log(d): Covariance matrix is not positive definite. Covariance matrix(0,0) is -1.#IND.\nIf the problem persists across multiple draws, you might have a problem with an initial state or a gradient somewhere.",
        "\n If the problem does not persist, the resulting samples will still be drawn from the posterior.\n\rIteration: 100 / 1000 [ 10%]  (Adapting)\rIteration: 200 / 1000 [ 20%]  (Adapting)\nInformational Message: The parameter state is about to be Metropolis rejected due to the following underlying, non-fatal (really) issue (and please ignore that what comes next might say 'error'): Error in function stan::prob::multi_normal_log(d): Covariance matrix is not positive definite. Covariance matrix(0,0) is 237.78545917547359.",
        "\nIf the problem persists across multiple draws, you might have a problem with an initial state or a gradient somewhere.\n If the problem does not persist, the resulting samples will still be drawn from the posterior.\n\rIteration: 300 / 1000 [ 30%]  (Adapting)\rIteration: 400 / 1000 [ 40%]  (Adapting)\rIteration: 500 / 1000 [ 50%]  (Adapting)\rIteration: 600 / 1000 [ 60%]  (Sampling)\rIteration: 700 / 1000 [ 70%]  (Sampling)\rIteration: 800 / 1000 [ 80%]  (Sampling)\rIteration: 900 / 1000 [ 90%]  (Sampling)\rIteration: ",
        "1000 / 1000 [100%]  (Sampling)\n\nSAMPLING FOR MODEL 'cvmlikelihood2D' NOW (CHAIN 3).\n\rIteration:   1 / 1000 [  0%]  (Adapting)\nInformational Message: The parameter state is about to be Metropolis rejected due to the following underlying, non-fatal (really) issue (and please ignore that what comes next might say 'error'): Error in function stan::prob::multi_normal_log(d): Covariance matrix is not positive definite. Covariance matrix(0,0) is 4.7713878813649896e+038.\nIf the problem persists across multiple draws, you might have",
        " a problem with an initial state or a gradient somewhere.\n If the problem does not persist, the resulting samples will still be drawn from the posterior.\n\rIteration: 100 / 1000 [ 10%]  (Adapting)\rIteration: 200 / 1000 [ 20%]  (Adapting)\rIteration: 300 / 1000 [ 30%]  (Adapting)\rIteration: 400 / 1000 [ 40%]  (Adapting)\rIteration: 500 / 1000 [ 50%]  (Adapting)\rIteration: 600 / 1000 [ 60%]  (Sampling)\rIteration: 700 / 1000 [ 70%]  (Sampling)\rIteration: 800 / 1000 [ 80%]  (Sampling)\rIteration: 900 / 1000 [ 90%] ",
        " (Sampling)\rIteration: 1000 / 1000 [100%]  (Sampling)\n\nSAMPLING FOR MODEL 'cvmlikelihood2D' NOW (CHAIN 4).\n\rIteration:   1 / 1000 [  0%]  (Adapting)\nInformational Message: The parameter state is about to be Metropolis rejected due to the following underlying, non-fatal (really) issue (and please ignore that what comes next might say 'error'): Error in function stan::prob::multi_normal_log(d): Covariance matrix is not positive definite. Covariance matrix(0,0) is -1.#IND.\nIf the problem persists across multiple draws, you might have",
        " a problem with an initial state or a gradient somewhere.\n If the problem does not persist, the resulting samples will still be drawn from the posterior.\n\nInformational Message: The parameter state is about to be Metropolis rejected due to the following underlying, non-fatal (really) issue (and please ignore that what comes next might say 'error'): Error in function stan::prob::multi_normal_log(d): Covariance matrix is not positive definite. Covariance matrix(0,0) is -1.#IND.\nIf the problem persists across multiple draws, you might have",
        " a problem with an initial state or a gradient somewhere.\n If the problem does not persist, the resulting samples will still be drawn from the posterior.\n\nInformational Message: The parameter state is about to be Metropolis rejected due to the following underlying, non-fatal (really) issue (and please ignore that what comes next might say 'error'): Error in function stan::prob::multi_normal_log(d): Covariance matrix is not positive definite. Covariance matrix(0,0) is -4.6955003590094648e+026.\nIf the problem persists across multiple draws, you might have",
        " a problem with an initial state or a gradient somewhere.\n If the problem does not persist, the resulting samples will still be drawn from the posterior.\n\rIteration: 100 / 1000 [ 10%]  (Adapting)\rIteration: 200 / 1000 [ 20%]  (Adapting)\rIteration: 300 / 1000 [ 30%]  (Adapting)\rIteration: 400 / 1000 [ 40%]  (Adapting)\rIteration: 500 / 1000 [ 50%]  (Adapting)\rIteration: 600 / 1000 [ 60%]  (Sampling)\rIteration: 700 / 1000 [ 70%]  (Sampling)\rIteration: 800 / 1000 [ 80%]  (Sampling)\rIteration: 900 / 1000 [ 90%] ",
        " (Sampling)\rIteration: 1000 / 1000 [100%]  (Sampling)\n\n",
        "> ",
        "CVM.fit",
        "Inference for Stan model: cvmlikelihood2D.\n4 chains, each with iter=1000; warmup=500; thin=1; \npost-warmup draws per chain=500, total post-warmup draws=2000.\n\n       mean se_mean  sd   2.5%    25%    50%    75%  97.5% n_eff Rhat\ntau     2.3     0.0 0.6    1.4    1.9    2.2    2.7    3.7   354    1\nnu      9.0     0.1 1.0    7.4    8.3    8.9    9.5   11.1   344    1\nlp__ -123.1     0.1 1.0 -125.8 -123.5 -122.7 -122.3 -122.0   389    1\n\nSamples were drawn using NUTS2 at Sun Oct 27 20:41:25 2013.\nFor each parameter, n_eff is a crude measure of effective sample size,\n",
        "and Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n",
        "> ",
        "tau.sim <- CVM.fit@sim$samples[[1]]$tau",
        "> ",
        "length(tau.sim)",
        "[1] 1000\n",
        "> ",
        "iter",
        "[1] 1000\n",
        "> ",
        "tau.sim[round(iter/2):iter]",
        "  [1] 2.639454 2.112121 2.112121 2.112121 2.112121 2.112121 2.112121 2.405877 2.405877 2.405877 2.405877 2.405877 3.038218 1.891795\n [15] 1.708290 1.708290 1.708290 1.708290 1.708290 2.047443 2.047443 1.772588 1.739080 1.459190 2.349092 2.349092 2.349092 2.349092\n [29] 1.738192 1.781173 2.213713 1.789385 1.922073 1.922073 1.742253 2.287801 2.454216 2.454216 2.454216 2.454216 2.454216 2.454216\n [43] 2.454216 1.756915 1.853050 1.853050 1.853050 1.853050 1.853050 1.853050 1.929770 1.929770 2.214756 2.214756 2.225263",
        " 2.225263\n [57] 2.161526 2.161526 1.771309 1.771309 2.484246 1.983713 1.923419 1.913155 2.413948 2.140341 2.037806 1.922889 1.922889 1.917933\n [71] 1.917933 1.912667 2.450684 2.132419 2.132419 2.346353 2.346353 1.619140 2.112587 2.112587 2.410804 2.412028 2.412028 3.053765\n [85] 3.053765 3.053765 3.053765 2.622415 3.095730 3.095730 3.095730 3.095730 3.095730 3.095730 3.095730 1.877719 1.877719 2.732978\n [99] 2.732978 2.732978 1.697004 1.145317 1.184052 1.088231 4.521074 3.174531 3.174531 2.826182 2.591536 1.960479",
        " 1.960479 1.960479\n[113] 2.547942 2.547942 2.547942 2.591694 1.745162 1.772353 2.531434 2.531434 1.957256 2.188206 2.462354 2.462354 2.462354 1.561274\n[127] 1.561274 1.561274 1.406839 2.512499 2.512499 1.944605 1.944605 1.944605 1.944605 1.863465 2.396683 2.396683 2.198763 2.626583\n[141] 2.626583 2.626583 2.626583 2.626583 2.626583 1.654847 2.010423 2.153399 2.193251 2.193251 2.193251 2.193251 1.937087 1.673679\n[155] 2.340894 2.604720 1.918664 1.918664 1.918664 2.294589 2.247812 2.247812 2.247812 2.247812 1.857789",
        " 1.958417 1.978976 2.272613\n[169] 1.951030 3.109745 3.109745 3.109745 3.123077 3.123077 3.123077 2.970676 2.970676 2.970676 3.132772 3.132772 3.132772 3.107996\n[183] 1.817171 1.949874 1.609190 1.625149 1.866747 1.866747 2.095597 2.095597 2.095597 2.095597 2.095597 2.095597 1.805629 1.781064\n[197] 1.781064 3.537047 3.537047 3.537047 2.845914 2.845914 2.845914 2.542375 2.445285 2.445285 2.248462 2.043321 2.177654 2.177654\n[211] 2.235859 2.266295 2.266295 2.415672 2.864079 2.803310 2.803310 2.803310 2.866883 2.866883",
        " 2.352813 2.352813 2.352813 2.352813\n[225] 2.205453 2.205453 1.970372 1.970372 1.970372 2.490390 1.788250 1.788250 1.788250 1.788250 2.416850 2.004039 2.004039 2.422912\n[239] 2.081994 2.600612 1.446814 2.567672 2.567672 2.235085 1.668590 1.668590 2.306558 2.155816 1.849675 2.163010 2.162626 1.630950\n[253] 1.630950 1.630950 2.450504 2.450504 2.178541 2.178541 2.178541 1.919441 1.919441 1.919441 1.919441 1.919441 1.915659 2.588594\n[267] 2.887843 2.232004 1.820216 2.105418 2.752808 2.151866 1.960765 2.497611 2.497611",
        " 2.497611 2.497611 2.497611 2.660001 2.627614\n[281] 2.723901 2.118269 2.118269 2.118269 2.118269 2.118269 2.363253 2.035816 1.716743 1.583110 1.688704 1.688704 3.147854 3.407369\n[295] 3.407369 3.745477 3.373670 3.373670 3.373670 2.808449 2.808449 2.808449 2.808449 2.897265 2.425235 1.783548 3.000367 2.059923\n[309] 1.886555 2.132554 2.608283 1.773219 2.968043 2.968043 1.857629 2.188858 2.188858 2.747918 2.747918 2.747918 2.747918 2.747918\n[323] 1.932700 1.932700 1.932700 1.996447 1.996447 1.697421 1.697421 1.617931",
        " 1.878792 1.878792 1.809579 1.925562 2.277733 2.066813\n[337] 2.066813 2.066813 2.066813 2.066813 2.410247 2.768448 2.505925 3.193525 3.193525 3.193525 2.900849 2.900849 1.864238 1.864238\n[351] 1.923510 1.837486 1.722963 1.718318 1.718318 1.718318 1.815488 1.723366 1.723366 1.632116 1.794247 1.794247 1.489428 1.489428\n[365] 1.489428 1.757885 1.757885 1.757885 1.559910 1.378572 2.268659 2.268659 2.268659 2.096014 2.096014 2.096014 2.173786 2.173786\n[379] 2.179645 2.179645 2.677076 2.677076 1.481366 2.957514 3.305125",
        " 3.305125 2.753914 3.072580 3.072580 2.997575 3.002658 1.723315\n[393] 1.915217 1.838769 1.838769 2.378772 2.950677 2.950677 2.950677 2.950677 3.608813 3.881183 3.881183 3.835681 3.835681 1.204032\n[407] 1.718465 2.231113 2.628187 3.333536 2.658058 2.658058 2.310203 2.002363 1.641047 1.870940 1.870940 1.870940 1.965240 1.674198\n[421] 2.553476 2.553476 1.931276 1.931276 2.111002 1.889405 1.889405 1.889405 1.889405 2.394795 2.394795 3.234249 3.290750 3.693616\n[435] 3.870589 3.870589 3.870589 3.870589 3.870589 3.461732",
        " 2.685171 2.685171 2.597507 2.651722 2.651722 1.876049 1.876049 2.278034\n[449] 2.278034 2.278034 1.831297 1.831297 1.831297 1.831297 1.831297 2.008831 2.425262 2.422932 2.422932 2.827647 2.572683 2.537561\n[463] 2.537561 2.923862 2.923862 2.418072 2.418072 2.319998 2.319998 2.319998 2.051013 2.051013 2.327064 2.327064 2.327064 2.327064\n[477] 2.168189 2.168189 2.241549 2.241549 2.241549 2.241549 2.051228 2.431440 2.285805 2.269202 3.139081 3.139081 3.139081 3.139081\n[491] 1.991303 2.531490 2.531490 2.531490 1.839757",
        " 3.258309 3.475063 2.659119 2.529221 2.820826 2.219696\n",
        "> ",
        "quantile(tau.sim[round(iter/2):iter])",
        "      0%      25%      50%      75%     100% \n1.088231 1.918664 2.232004 2.626583 4.521074 \n",
        "> ",
        "CVM.fit",
        "Inference for Stan model: cvmlikelihood2D.\n4 chains, each with iter=1000; warmup=500; thin=1; \npost-warmup draws per chain=500, total post-warmup draws=2000.\n\n       mean se_mean  sd   2.5%    25%    50%    75%  97.5% n_eff Rhat\ntau     2.3     0.0 0.6    1.4    1.9    2.2    2.7    3.7   354    1\nnu      9.0     0.1 1.0    7.4    8.3    8.9    9.5   11.1   344    1\nlp__ -123.1     0.1 1.0 -125.8 -123.5 -122.7 -122.3 -122.0   389    1\n\nSamples were drawn using NUTS2 at Sun Oct 27 20:41:25 2013.\nFor each parameter, n_eff is a crude measure of effective sample size,\n",
        "and Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n",
        "> ",
        "mean(tau.sim[round(iter/2):iter])",
        "[1] 2.324233\n",
        "> ",
        "tau.CI <- quantile(tau.sim[round(iter/2):iter], c(0.025, 0.975))",
        "> ",
        "tau.CI",
        "    2.5%    97.5% \n1.560592 3.572930 \n",
        "> ",
        "tau.CI <- quantile(tau.sim[round(iter/5):iter], c(0.025, 0.975))",
        "> ",
        "tau.CI",
        "    2.5%    97.5% \n1.489428 3.537047 \n",
        "> ",
        "plot(tau.sim, type=\"l\")",
        "> ",
        "      hist(tau.sim[-(1:100)], col=\"grey\", bor=\"darkgrey\")",
        "> ",
        "      abline(v=quantile(tau.sim[-(1:100)]), lty=c(3,2,1,2,3), col=2, lwd=2)",
        "> ",
        "      plot(nu.sim, type=\"l\")",
        "Error in plot(nu.sim, type = \"l\") : \n  error in evaluating the argument 'x' in selecting a method for function 'plot': Error: object 'nu.sim' not found\n",
        "> ",
        "      hist(nu.sim[-(1:100)], col=\"grey\", bor=\"darkgrey\")",
        "Error in hist(nu.sim[-(1:100)], col = \"grey\", bor = \"darkgrey\") : \n  object 'nu.sim' not found\n",
        "> ",
        "      abline(v=quantile(nu.sim[-(1:100)]), lty=c(3,2,1,2,3), col=2, lwd=2)",
        "Error in quantile(nu.sim[-(1:100)]) : object 'nu.sim' not found\n",
        "> ",
        "layout(rbind(c(1,1,2),c(3,3,4)))",
        "> ",
        "tau.sim <- CVM.fit@sim$samples[[1]]$tau",
        "> ",
        "nu.sim <- CVM.fit@sim$samples[[1]]$nu",
        "> ",
        "plot(tau.sim, type=\"l\")",
        "> ",
        "hist(tau.sim[-(1:100)], col=\"grey\", bor=\"darkgrey\")",
        "> ",
        "abline(v=quantile(tau.sim[-(1:100)]), lty=c(3,2,1,2,3), col=2, lwd=2)",
        "> ",
        "plot(nu.sim, type=\"l\")",
        "> ",
        "hist(nu.sim[-(1:100)], col=\"grey\", bor=\"darkgrey\")",
        "> ",
        "abline(v=quantile(nu.sim[-(1:100)]), lty=c(3,2,1,2,3), col=2, lwd=2)",
        "> ",
        "layout(rbind(c(1,1,2),c(3,3,4)))",
        "> ",
        "      tau.sim <- CVM.fit@sim$samples[[1]]$tau",
        "> ",
        "      nu.sim <- CVM.fit@sim$samples[[1]]$nu",
        "> ",
        "      ",
        "> ",
        "      plot(tau.sim, type=\"l\")",
        "> ",
        "      hist(tau.sim[50:iter], col=\"grey\", bor=\"darkgrey\")",
        "> ",
        "      abline(v=quantile(tau.sim[50:iter]), lty=c(3,2,1,2,3), col=2, lwd=2)",
        "> ",
        "      plot(nu.sim, type=\"l\")",
        "> ",
        "      hist(nu.sim[50:iter], col=\"grey\", bor=\"darkgrey\")",
        "> ",
        "      abline(v=quantile(nu.sim[50:iter]), lty=c(3,2,1,2,3), col=2, lwd=2)",
        "> ",
        "par(bty=\"l\")",
        "> ",
        "tau.sim <- CVM.fit@sim$samples[[1]]$tau",
        "> ",
        "nu.sim <- CVM.fit@sim$samples[[1]]$nu",
        "> ",
        "plot(tau.sim, type=\"l\")",
        "> ",
        "hist(tau.sim[50:iter], col=\"grey\", bor=\"darkgrey\")",
        "> ",
        "abline(v=quantile(tau.sim[50:iter]), lty=c(3,2,1,2,3), col=2, lwd=2)",
        "> ",
        "plot(nu.sim, type=\"l\")",
        "> ",
        "hist(nu.sim[50:iter], col=\"grey\", bor=\"darkgrey\")",
        "> ",
        "abline(v=quantile(nu.sim[50:iter]), lty=c(3,2,1,2,3), col=2, lwd=2)",
        "> ",
        "plot(tau.sim, type=\"l\", main=\"tau chain\")",
        "> ",
        "      hist(tau.sim[50:iter], col=\"grey\", bor=\"darkgrey\")",
        "> ",
        "      abline(v=quantile(tau.sim[50:iter]), lty=c(3,2,1,2,3), col=2, lwd=2)",
        "> ",
        "      plot(nu.sim, type=\"l\", main=\"nu chain\"",
        "+ ",
        "      hist(nu.sim[50:iter], col=\"grey\", bor=\"darkgrey\")",
        "Error: unexpected symbol in:\n\"      plot(nu.sim, type=\"l\", main=\"nu chain\"\n      hist\"\n",
        "> ",
        "      abline(v=quantile(nu.sim[50:iter]), lty=c(3,2,1,2,3), col=2, lwd=2)",
        "> ",
        "plot(nu.sim, type=\"l\", main=\"nu chain\")",
        "> ",
        "hist(nu.sim[50:iter], col=\"grey\", bor=\"darkgrey\")",
        "> ",
        "abline(v=quantile(nu.sim[50:iter]), lty=c(3,2,1,2,3), col=2, lwd=2)",
        "> ",
        "quantile",
        "function (x, ...) \nUseMethod(\"quantile\")\n<bytecode: 0x00000000095c50f8>\n<environment: namespace:stats>\n",
        "> ",
        "layout(rbind(c(1,1,2),c(3,3,4)))",
        "> ",
        "      par(bty=\"l\")",
        "> ",
        "      tau.sim <- CVM.fit@sim$samples[[1]]$tau",
        "> ",
        "      nu.sim <- CVM.fit@sim$samples[[1]]$nu",
        "> ",
        "      ",
        "> ",
        "      p <- c(0.025, 0.25, 0.5, 0.75, 0.975)",
        "> ",
        "      ",
        "> ",
        "      plot(tau.sim, type=\"l\", main=\"tau chain\")",
        "> ",
        "      hist(tau.sim[50:iter], col=\"grey\", bor=\"darkgrey\")",
        "> ",
        "      abline(v=quantile(tau.sim[50:iter], p), lty=c(3,2,1,2,3), col=2, lwd=2)",
        "> ",
        "      plot(nu.sim, type=\"l\", main=\"nu chain\")",
        "> ",
        "      hist(nu.sim[50:iter], col=\"grey\", bor=\"darkgrey\")",
        "> ",
        "      abline(v=quantile(nu.sim[50:iter], p), lty=c(3,2,1,2,3), col=2, lwd=2)",
        "> ",
        "tau.hat <- mean(tau.sim[50:iter])",
        "> ",
        "tau.CI <- quantile(tau.sim[50:iter], c(0.025, 0.975))",
        "> ",
        "tau.sim <- CVM.fit@sim$samples[[1]]$tau",
        "> ",
        "tau.hat <- mean(tau.sim[50:iter])",
        "> ",
        "tau.CI <- quantile(tau.sim[50:iter], c(0.025, 0.975))",
        "> ",
        "nu.sim <- CVM.fit@sim$samples[[1]]$nu",
        "> ",
        "nu.hat <- mean(nu.sim[50:iter])",
        "> ",
        "nu.CI <- quantile(nu.sim[50:iter], c(0.025, 0.975))",
        "> ",
        "results <- data.frame(rbind(c(tau.hat, tau.CI), c(nu.hat, nu.CI)))",
        "> ",
        "results",
        "        V1    X2.5.    X97.5.\n1 2.302909 1.488910  3.537047\n2 8.936476 7.429572 11.034265\n\nRestarting R session...\n\n",
        "> ",
        "library(cvm)",
        "\nRestarting R session...\n\n",
        "> ",
        "library(cvm)",
        "> ",
        "CVM.fit <- NULL",
        "> ",
        "rm(a)",
        "Warning message:\n",
        "In rm(a) : object 'a' not found\n",
        "> ",
        "plot(a)",
        "Error in plot(a) : \n  error in evaluating the argument 'x' in selecting a method for function 'plot': Error: object 'a' not found\n",
        "> ",
        "try(plot(a))",
        "Error in plot(a) : \n  error in evaluating the argument 'x' in selecting a method for function 'plot': Error: object 'a' not found\n\n",
        "> ",
        "a <- try(plot(a))",
        "Error in plot(a) : \n  error in evaluating the argument 'x' in selecting a method for function 'plot': Error: object 'a' not found\n\n",
        "> ",
        "a",
        "[1] \"Error in plot(a) : \\n  error in evaluating the argument 'x' in selecting a method for function 'plot': Error: object 'a' not found\\n\\n\"\nattr(,\"class\")\n[1] \"try-error\"\nattr(,\"condition\")\n<simpleError in plot(a): error in evaluating the argument 'x' in selecting a method for function 'plot': Error: object 'a' not found\n>\n",
        "> ",
        "is(a)",
        "[1] \"try-error\"\n",
        "> ",
        "CVM.fit <- NULL",
        "> ",
        "is(CVM.fit) != try.error",
        "Error: object 'try.error' not found\n",
        "> ",
        "is(CVM.fit) != \"try.error\"",
        "[1] TRUE TRUE TRUE\n",
        "> ",
        "CVM.fit",
        "NULL\n",
        "> ",
        "is(a) == \"try.error\"",
        "[1] FALSE\n",
        "> ",
        "is(a) == \"try-error\"",
        "[1] TRUE\n",
        "> ",
        "is(CVM.fit) != \"try-error\"",
        "[1] TRUE TRUE TRUE\n",
        "> ",
        "is(CVM.fit)[1] != \"try-error\"",
        "[1] TRUE\n\nRestarting R session...\n\n",
        "> ",
        "library(cvm)",
        "\nRestarting R session...\n\n",
        "> ",
        "library(cvm)",
        "\nRestarting R session...\n\n",
        "> ",
        "library(cvm)",
        "\nRestarting R session...\n\n",
        "> ",
        "library(cvm)",
        "\nRestarting R session...\n\n",
        "> ",
        "library(cvm)",
        "> ",
        "CVM.fit <- try( sampling(CVMstan2D, data = CVM.data, iter = iter, pars=c(\"tau\", \"nu\"), chains = chains))",
        "SAMPLING FOR MODEL 'cvmlikelihood2D' NOW (CHAIN 1).\n\rIteration:   1 / 1000 [  0%]  (Adapting)\nInformational Message: The parameter state is about to be Metropolis rejected due to the following underlying, non-fatal (really) issue (and please ignore that what comes next might say 'error'): Error in function stan::prob::multi_normal_log(d): Covariance matrix is not positive definite. Covariance matrix(0,0) is 1.254569772657133e-036.\nIf the problem persists across multiple draws, you might have a problem with an initial state or a gradient somewhere.",
        "\n If the problem does not persist, the resulting samples will still be drawn from the posterior.\n\nInformational Message: The parameter state is about to be Metropolis rejected due to the following underlying, non-fatal (really) issue (and please ignore that what comes next might say 'error'): Error in function stan::prob::multi_normal_log(d): Covariance matrix is not positive definite. Covariance matrix(0,0) is 1.765351898010797e-014.\nIf the problem persists across multiple draws, you might have a problem with an initial state or a gradient somewhere.",
        "\n If the problem does not persist, the resulting samples will still be drawn from the posterior.\n\rIteration: 100 / 1000 [ 10%]  (Adapting)\rIteration: 200 / 1000 [ 20%]  (Adapting)\rIteration: 300 / 1000 [ 30%]  (Adapting)\rIteration: 400 / 1000 [ 40%]  (Adapting)\rIteration: 500 / 1000 [ 50%]  (Adapting)\rIteration: 600 / 1000 [ 60%]  (Sampling)\rIteration: 700 / 1000 [ 70%]  (Sampling)\rIteration: 800 / 1000 [ 80%]  (Sampling)\rIteration: 900 / 1000 [ 90%]  (Sampling)\rIteration: 1000 / 1000 [100%]  (Sampling)\n\nSAMPLING",
        " FOR MODEL 'cvmlikelihood2D' NOW (CHAIN 2).\n\rIteration:   1 / 1000 [  0%]  (Adapting)\nInformational Message: The parameter state is about to be Metropolis rejected due to the following underlying, non-fatal (really) issue (and please ignore that what comes next might say 'error'): Error in function stan::prob::multi_normal_log(d): Covariance matrix is not positive definite. Covariance matrix(0,0) is -1.#IND.\nIf the problem persists across multiple draws, you might have a problem with an initial state or a gradient somewhere.",
        "\n If the problem does not persist, the resulting samples will still be drawn from the posterior.\n\nInformational Message: The parameter state is about to be Metropolis rejected due to the following underlying, non-fatal (really) issue (and please ignore that what comes next might say 'error'): Error in function stan::prob::multi_normal_log(d): Covariance matrix is not positive definite. Covariance matrix(0,0) is -1.#IND.\nIf the problem persists across multiple draws, you might have a problem with an initial state or a gradient somewhere.",
        "\n If the problem does not persist, the resulting samples will still be drawn from the posterior.\n\nInformational Message: The parameter state is about to be Metropolis rejected due to the following underlying, non-fatal (really) issue (and please ignore that what comes next might say 'error'): Error in function stan::prob::multi_normal_log(d): Covariance matrix is not positive definite. Covariance matrix(0,0) is 7.0321840562464827e+048.\nIf the problem persists across multiple draws, you might have a problem with an initial state or a gradient somewhere.",
        "\n If the problem does not persist, the resulting samples will still be drawn from the posterior.\n\nInformational Message: The parameter state is about to be Metropolis rejected due to the following underlying, non-fatal (really) issue (and please ignore that what comes next might say 'error'): Error in function stan::prob::multi_normal_log(d): Covariance matrix is not positive definite. Covariance matrix(0,0) is -352731.3121102326.\nIf the problem persists across multiple draws, you might have a problem with an initial state or a gradient somewhere.",
        "\n If the problem does not persist, the resulting samples will still be drawn from the posterior.\n\rIteration: 100 / 1000 [ 10%]  (Adapting)\rIteration: 200 / 1000 [ 20%]  (Adapting)\rIteration: 300 / 1000 [ 30%]  (Adapting)\rIteration: 400 / 1000 [ 40%]  (Adapting)\rIteration: 500 / 1000 [ 50%]  (Adapting)\rIteration: 600 / 1000 [ 60%]  (Sampling)",
        "\n",
        "> ",
        "chains <- 1",
        "> ",
        "item <- 200",
        "> ",
        "CVM.fit <- try(sampling(CVMstan2D, data = CVM.data, iter = iter, pars=c(\"tau\", \"nu\"), chains = chains))",
        "SAMPLING FOR MODEL 'cvmlikelihood2D' NOW (CHAIN 1).\n\rIteration:   1 / 1000 [  0%]  (Adapting)\nInformational Message: The parameter state is about to be Metropolis rejected due to the following underlying, non-fatal (really) issue (and please ignore that what comes next might say 'error'): Error in function stan::prob::multi_normal_log(d): Covariance matrix is not positive definite. Covariance matrix(0,0) is -1.#IND.\nIf the problem persists across multiple draws, you might have a problem with an initial state or a gradient somewhere.",
        "\n If the problem does not persist, the resulting samples will still be drawn from the posterior.\n\nInformational Message: The parameter state is about to be Metropolis rejected due to the following underlying, non-fatal (really) issue (and please ignore that what comes next might say 'error'): Error in function stan::prob::multi_normal_log(d): Covariance matrix is not positive definite. Covariance matrix(0,0) is -1.#IND.\nIf the problem persists across multiple draws, you might have a problem with an initial state or a gradient somewhere.",
        "\n If the problem does not persist, the resulting samples will still be drawn from the posterior.\n\nInformational Message: The parameter state is about to be Metropolis rejected due to the following underlying, non-fatal (really) issue (and please ignore that what comes next might say 'error'): Error in function stan::prob::multi_normal_log(d): Covariance matrix is not positive definite. Covariance matrix(0,0) is 6.9930210777170162e+040.\nIf the problem persists across multiple draws, you might have a problem with an initial state or a gradient somewhere.",
        "\n If the problem does not persist, the resulting samples will still be drawn from the posterior.\n\rIteration: 100 / 1000 [ 10%]  (Adapting)\rIteration: 200 / 1000 [ 20%]  (Adapting)\rIteration: 300 / 1000 [ 30%]  (Adapting)\nInformational Message: The parameter state is about to be Metropolis rejected due to the following underlying, non-fatal (really) issue (and please ignore that what comes next might say 'error'): Error in function stan::prob::multi_normal_log(d): Covariance matrix is not positive definite. Covariance matrix(0,0) is 12.010232197748662.",
        "\nIf the problem persists across multiple draws, you might have a problem with an initial state or a gradient somewhere.\n If the problem does not persist, the resulting samples will still be drawn from the posterior.\n\rIteration: 400 / 1000 [ 40%]  (Adapting)\rIteration: 500 / 1000 [ 50%]  (Adapting)",
        "\n",
        "> ",
        "iter",
        "[1] 1000\n",
        "> ",
        "iter <- 200",
        "> ",
        "CVM.fit <- try(sampling(CVMstan2D, data = CVM.data, iter = iter, pars=c(\"tau\", \"nu\"), chains = chains))",
        "SAMPLING FOR MODEL 'cvmlikelihood2D' NOW (CHAIN 1).\n\rIteration:   1 / 200 [  0%]  (Adapting)\nInformational Message: The parameter state is about to be Metropolis rejected due to the following underlying, non-fatal (really) issue (and please ignore that what comes next might say 'error'): Error in function stan::prob::multi_normal_log(d): Covariance matrix is not positive definite. Covariance matrix(0,0) is 1.13978152694028.\nIf the problem persists across multiple draws, you might have a problem with an initial state or a gradient somewhere.",
        "\n If the problem does not persist, the resulting samples will still be drawn from the posterior.\n\rIteration:  20 / 200 [ 10%]  (Adapting)\rIteration:  40 / 200 [ 20%]  (Adapting)\rIteration:  60 / 200 [ 30%]  (Adapting)\rIteration:  80 / 200 [ 40%]  (Adapting)\rIteration: 100 / 200 [ 50%]  (Adapting)\rIteration: 120 / 200 [ 60%]  (Sampling)\rIteration: 140 / 200 [ 70%]  (Sampling)\rIteration: 160 / 200 [ 80%]  (Sampling)\rIteration: 180 / 200 [ 90%]  (Sampling)\rIteration: 200 / 200 [100%]  (Sampling)\n\n",
        "> ",
        "is(CVM.fit)[1] != \"try-error\"",
        "[1] TRUE\n",
        "> ",
        "if(is(CVM.fit)[1] != \"try-error\") break()",
        "Error: no loop for break/next, jumping to top level\n",
        "\nRestarting R session...\n\n",
        "> ",
        "library(cvm)",
        "Error in as.environment(pos) : invalid 'pos' argument\n",
        "\nRestarting R session...\n\n",
        "> ",
        "library(cvm)",
        "Loading required package: rstan\n",
        "rstan (Version 1.3.0, packaged: 2013-04-12 21:12:02 UTC, GitRev: f57455593d14)\n",
        "> ",
        "setwd(\"c:/eli/programming/cvm/R\")",
        "> ",
        "require(rstan)",
        "> ",
        "system.time(CVMstan2D <- stan_model(file = \"../STAN/cvmlikelihood2D.stan\"))",
        "Error in file(fname, \"rt\") : cannot open the connection\n",
        "In addition: ",
        "Warning message:\n",
        "In file(fname, \"rt\") :\n  cannot open file '../STAN/cvmlikelihood2D.stan': No such file or directory\n",
        "Error in get_model_strcode(file, model_code) : \n  cannot open model file \"../STAN/cvmlikelihood2D.stan\"\n",
        "Timing stopped at: 0.02 0 0.05 \n",
        "> ",
        "system.time(CVMstan2D <- stan_model(file = \"/cvmlikelihood2D.stan\"))",
        "Error in file(fname, \"rt\") : cannot open the connection\n",
        "In addition: ",
        "Warning message:\n",
        "In file(fname, \"rt\") :\n  cannot open file '/cvmlikelihood2D.stan': No such file or directory\n",
        "Error in get_model_strcode(file, model_code) : \n  cannot open model file \"/cvmlikelihood2D.stan\"\n",
        "Timing stopped at: 0 0 0 \n",
        "> ",
        "system.time(CVMstan2D <- stan_model(file = \"cvmlikelihood2D.stan\"))",
        "\nTRANSLATING MODEL 'cvmlikelihood2D' FROM Stan CODE TO C++ CODE NOW.\nCOMPILING THE C++ CODE FOR MODEL 'cvmlikelihood2D' NOW.\ncygwin warning:\n  MS-DOS style path detected: C:/PROGRA~1/R/R-30~1.1/etc/x64/Makeconf\n  Preferred POSIX equivalent is: /cygdrive/c/PROGRA~1/R/R-30~1.1/etc/x64/Makeconf\n  CYGWIN environment variable option \"nodosfilewarning\" turns off this warning.\n  Consult the user's guide for more details about POSIX paths:\n    http://cygwin.com/cygwin-ug-net/using.html#using-pathnames\nIn file included from C:/eli/R/win-library/3.0/rstan/include/rstan/rstaninc.hpp:3:0,\n",
        "                 from file9b47cc854c5.cpp:6:\nC:/eli/R/win-library/3.0/rstan/include/rstan/stan_fit.hpp: In constructor 'rstan::stan_fit<Model, RNG>::stan_fit(SEXP, SEXP) [with Model = model9b42b605b17_cvmlikelihood2D_namespace::model9b42b605b17_cvmlikelihood2D, RNG = boost::random::additive_combine_engine<boost::random::linear_congruential_engine<unsigned int, 40014u, 0u, 2147483563u>, boost::random::linear_congruential_engine<unsigned int, 40692u, 0u, 2147483399u> >, SEXP = SEXPREC*]':\nC:/eli/R/win-library/3.0/Rcpp/include/Rcpp/module/Module_generated_Constructor.h:60:13:   instantiated from 'Class* Rcpp::Constructor_2<Class, U0, U1>::get_new(SEXPREC**, int) [with Class = rstan::stan_fit<model9b42b605b17_cvmlikelihood2D_namespace::model9b42b605b17_cvmlikelihood2D, boost::random::additive_combine_engine<boost::random::linear_congruential_engine<unsigned int, 40014u, 0u, 2147483563u>, boost::random::linear_congruential_engine<unsigned int, 40692u, 0u, 2147483399u> > >, U0 = SEXPREC*, U1 = SEXPREC*, SEXP = SEXPREC*]'\n",
        "file9b47cc854c5.cpp:553:1:   instantiated from here\nC:/eli/R/win-library/3.0/rstan/include/rstan/stan_fit.hpp:720:18: warning: 'rstan::stan_fit<model9b42b605b17_cvmlikelihood2D_namespace::model9b42b605b17_cvmlikelihood2D, boost::random::additive_combine_engine<boost::random::linear_congruential_engine<unsigned int, 40014u, 0u, 2147483563u>, boost::random::linear_congruential_engine<unsigned int, 40692u, 0u, 2147483399u> > >::num_params2_' will be initialized after [-Wreorder]\nC:/eli/R/win-library/3.0/rstan/include/rstan/stan_fit.hpp:711:9: warning:   'boost::random::additive_combine_engine<boost::random::linear_congruential_engine<unsigned int, 40014u, 0u, 2147483563u>, boost::random::linear_congruential_engine<unsigned int, 40692u, 0u, 2147483399u> > rstan::stan_fit<model9b42b605b17_cvmlikelihood2D_namespace::model9b42b605b17_cvmlikelihood2D, boost::random::additive_combine_engine<boost::random::linear_congruential_engine<unsigned int, 40014u, 0u, 2147483563u>, boost::random::linear_congruential_engine<unsigned int, 40692u, 0u, 2147483399u> > >::base_rng' [-Wreorder]\n",
        "C:/eli/R/win-library/3.0/rstan/include/rstan/stan_fit.hpp:779:5: warning:   when initialized here [-Wreorder]\nC:/eli/R/win-library/3.0/rstan/include//stansrc/stan/agrad/rev/var_stack.hpp: At global scope:\nC:/eli/R/win-library/3.0/rstan/include//stansrc/stan/agrad/rev/var_stack.hpp:49:17: warning: 'void stan::agrad::free_memory()' defined but not used [-Wunused-function]\n   user  system elapsed \n   0.12    0.03  127.41 \n",
        "> ",
        "save(CVMstan2D, file=\"../cvm/data/CVMstan2D.Rda\")",
        "\nRestarting R session...\n\n\nRestarting R session...\n\n",
        "> ",
        "library(cvm)",
        "> ",
        "CVM.fit",
        "Inference for Stan model: cvmlikelihood2D.\n1 chains, each with iter=200; warmup=100; thin=1; \npost-warmup draws per chain=100, total post-warmup draws=100.\n\n       mean se_mean  sd   2.5%    25%    50%    75%  97.5% n_eff Rhat\ntau     2.2     0.1 0.5    1.3    1.9    2.2    2.4    3.2    35    1\nnu      8.8     0.1 0.7    7.4    8.3    8.8    9.2   10.0    29    1\nlp__ -122.9     0.1 1.0 -125.6 -123.2 -122.6 -122.2 -122.1    54    1\n\nSamples were drawn using NUTS2 at Sun Oct 27 21:35:34 2013.\nFor each parameter, n_eff is a crude measure of effective sample size,\n",
        "and Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n",
        "> ",
        "source('~/programming/cvm/cvm/R/diagnoseSTANfit.R', echo=TRUE)",
        "\n> #' Diagnose STAN fit\n> #' \n> #' Used internally in \\code\\link{estimateCVM.fullL}\n> #' \n> #' @details provides diagnosis plots of chain convergence a .... [TRUNCATED] \n",
        "> ",
        "diagnoseSTANfit(CVM.fit)",
        "\nRestarting R session...\n\n\nRestarting R session...\n\n\nRestarting R session...\n\n\nRestarting R session...\n\n"
    ],
    "type" : [
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        2,
        0,
        1,
        2,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        2,
        0,
        1,
        0,
        1,
        0,
        1,
        3,
        0,
        1,
        2,
        0,
        1,
        2,
        0,
        1,
        0,
        1,
        0,
        1,
        3,
        0,
        1,
        2,
        0,
        1,
        2,
        0,
        1,
        3,
        0,
        1,
        3,
        0,
        1,
        2,
        0,
        1,
        3,
        0,
        1,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        2,
        0,
        1,
        0,
        1,
        0,
        1,
        3,
        0,
        1,
        2,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        2,
        0,
        1,
        2,
        0,
        1,
        2,
        0,
        1,
        0,
        1,
        2,
        0,
        1,
        2,
        0,
        1,
        0,
        1,
        0,
        1,
        3,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        3,
        0,
        1,
        3,
        0,
        1,
        3,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        2,
        0,
        1,
        0,
        1,
        2,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        2,
        0,
        1,
        0,
        1,
        3,
        0,
        1,
        3,
        0,
        1,
        2,
        0,
        1,
        2,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        2,
        0,
        1,
        0,
        1,
        0,
        1,
        2,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        2,
        0,
        1,
        2,
        0,
        1,
        0,
        1,
        2,
        0,
        1,
        2,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        2,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        2,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        2,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        2,
        0,
        1,
        2,
        0,
        1,
        0,
        1,
        2,
        0,
        1,
        2,
        0,
        1,
        2,
        0,
        1,
        3,
        3,
        2,
        0,
        1,
        0,
        1,
        2,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        3,
        0,
        1,
        0,
        1,
        0,
        1,
        3,
        0,
        1,
        0,
        1,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        0,
        1,
        2,
        2,
        0,
        1,
        0,
        1,
        2,
        0,
        1,
        2,
        0,
        1,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        0,
        1,
        2,
        0,
        1,
        2,
        2,
        0,
        1,
        2,
        0,
        1,
        0,
        1,
        2,
        0,
        1,
        0,
        1,
        2,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        3,
        0,
        1,
        3,
        0,
        1,
        3,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        3,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        2,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        2,
        0,
        1,
        2,
        0,
        1,
        0,
        1,
        0,
        1,
        3,
        3,
        0,
        1,
        3,
        0,
        1,
        3,
        0,
        1,
        3,
        0,
        1,
        2,
        0,
        1,
        2,
        0,
        1,
        0,
        1,
        3,
        0,
        1,
        2,
        0,
        1,
        2,
        0,
        1,
        2,
        0,
        1,
        2,
        0,
        1,
        2,
        0,
        1,
        2,
        0,
        1,
        2,
        0,
        1,
        2,
        0,
        1,
        2,
        0,
        1,
        2,
        0,
        1,
        0,
        1,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        3,
        0,
        1,
        0,
        1,
        0,
        1,
        2,
        2,
        2,
        2,
        2,
        3,
        0,
        1,
        2,
        0,
        1,
        0,
        1,
        2,
        2,
        0,
        1,
        2,
        0,
        1,
        3,
        2,
        0,
        1,
        3,
        2,
        0,
        1,
        3,
        3,
        0,
        1,
        0,
        1,
        0,
        1,
        3,
        3,
        3,
        3,
        3,
        2,
        0,
        1,
        3,
        3,
        3,
        3,
        3,
        2,
        0,
        1,
        2,
        2,
        2,
        2,
        0,
        1,
        2,
        0,
        1,
        0,
        1,
        2,
        2,
        0,
        1,
        2,
        0,
        1,
        2
    ]
}